# PRD - MODEL

---

> Covers scoring architecture, model components, datasets, instrumentation, and feature-to-model mapping. This is a sub-document of the Overall PRD.
> 

---

## 1. What the Model Does

Takes a complete session trace as input and produces a structured behavioral assessment as output. The session trace is a timestamped log of all candidate activity — prompts sent, AI responses, code edits, file navigation, terminal executions. The output is a score vector across rubric dimensions plus an overall behavioral label and a human-readable qualitative report.

**Input:** Sequence of timestamped events `{ timestamp, actor, event_type, content, metadata }`

**Output:**

- Score 1–5 per rubric dimension
- Overall label: `over-reliant` / `balanced` / `strategic`
- Qualitative narrative explaining scores with specific session examples

The model does not evaluate whether the code is correct — that is handled by the functional scoring pipeline (test runner). The model evaluates how the candidate worked.

---

## 2. Architecture Overview

Three components run in sequence. Components 1 and 2 are trained models targeting distinct signal types. Component 3 is a heuristic. The final layer is an LLM judge that produces the qualitative narrative.

```
Session Trace
      │
      ├──► [Component 1] Structural Behavior Classifier
      │         XGBoost on extracted log features
      │         Trained on CUPS (Copilot telemetry)
      │         → structural score vector
      │
      ├──► [Component 2] Semantic Prompt Quality Scorer
      │         Gradient boosted classifier + CodeBERT embeddings (inference only)
      │         Trained on WildChat coding subset
      │         → prompt quality score vector
      │
      ├──► [Component 3] Critical Review Detector
      │         Normalized edit distance heuristic
      │         Thresholds calibrated from CUPS post-acceptance edit rates
      │         → review quality score per AI interaction
      │
      ▼
[Aggregation Layer]
      Weighted average, weights derived from Component 1 feature importances
      → final dimension scores + overall label
      │
      ▼
[LLM Judge — Claude]
      Prompted with scores + session excerpts
      → qualitative narrative report
```

---

## 3. Rubric Dimensions

These are the dimensions scored in the final output. Each maps to one or more model components.

| Dimension | Description | Primary Component |
| --- | --- | --- |
| Verification frequency | How often candidate ran code vs. just writing | Component 1 |
| Re-prompt vs. debug ratio | When code fails, thinks vs. immediately re-prompts | Component 1 |
| Time distribution | Time split across reading, writing, prompting phases | Component 1 |
| Iteration depth | Edit cycles before submission | Component 1 |
| Test coverage | Did candidate write their own tests | Component 1 |
| Codebase exploration | Read relevant files before writing anything | Component 1 |
| Prompt granularity | Targeted prompts vs. full delegation | Component 2 |
| Prompt context quality | Included relevant code/context vs. asked blindly | Component 2 |
| Problem decomposition | Broke task down before implementing | Component 2 |
| Conceptual understanding | Prompts reflect genuine codebase understanding | Component 2 |
| AI output modification | Changed AI suggestions meaningfully vs. paste-as-is | Component 3 |
| Critical review quality | Edits reflect actual code review vs. cosmetic changes | Component 3 |

---

## 4. Component 1 — Structural Behavior Classifier

### What it does

Classifies behavioral patterns from quantitative session log features. Handles all metrics that are directly measurable as numbers or ratios from the event stream.

### Architecture

**XGBoost classifier** on a fixed-length feature vector extracted from the session log.

XGBoost is chosen over a neural approach here because:

- Feature set is small and tabular, not sequential raw data
- Non-linear feature interactions matter (high re-prompt rate is only bad combined with low execution frequency)
- Produces feature importance scores useful for aggregation layer weighting and report generation
- Trains in seconds on small datasets

### Input Features

| Feature | Derivation |
| --- | --- |
| Verification frequency | Count of execution events / total session time |
| Re-prompt ratio | AI prompts sent within 60s of failed execution / total failures |
| Time in editor | Cumulative time with editor panel focused |
| Time in chat | Cumulative time with AI chat panel focused |
| Time in file tree | Cumulative time navigating files |
| Orientation time | Time before first edit event |
| Iteration count | Number of distinct edit-run cycles |
| Test lines written | Lines matching test function patterns in candidate's own edits |
| File open count | Unique files opened before first edit |
| Dwell time per file | Average time cursor stayed in each file before closing |
| Prompt count | Total prompts sent |
| Prompt rate | Prompts per minute |
| Acceptance rate | Proportion of AI suggestions accepted (mapped from CUPS) |
| Deliberation time | Mean pause duration before accepting AI output |
| Post-acceptance edit rate | Proportion of accepted suggestions subsequently modified |

### Training Data

**CUPS (microsoft/coderec_programming_states)** — real telemetry from programmers using GitHub Copilot. Logs fine-grained interactions including suggestions shown, accepted, rejected, and ignored, plus cognitive state annotations (thinking, pausing, typing). Proxy behavioral labels derived from the combination of acceptance rate, deliberation time, and post-acceptance edit rate: sessions with low blind acceptance + high pause states + post-edit modifications = strategic; high blind acceptance + minimal pause + no modification = over-reliant.

Supplemented with **synthetic labeled sessions** (see Section 8) for direct end-to-end label supervision.

### Domain Gap Note

CUPS captures inline Copilot suggestion behavior rather than chat-based AI sessions. The underlying behavioral construct — deliberation before accepting AI output, modification vs. paste-as-is — is consistent across interfaces. CUPS is the best available real-world telemetry proxy; in-domain session data replaces it post-hackathon.

### Output

Score vector on structural dimensions (1–5 per dimension) + feature importance weights passed to aggregation layer and LLM judge.

---

## 5. Component 2 — Semantic Prompt Quality Scorer

### What it does

Classifies the semantic quality of candidate prompts. Detects whether prompts are vague vs. specific, context-rich vs. context-free, targeted vs. over-delegating, and whether they reflect genuine codebase understanding.

### Architecture

**Gradient boosted classifier** (LightGBM) on engineered features from prompt text, optionally augmented with **CodeBERT embeddings** (pretrained inference only — no fine-tuning).

Feature set per prompt:

- Prompt length and structure
- Presence of code context (backtick/code block detection, function name references, file path references)
- Specificity signals: constraint language, scoped verbs ("modify" vs. "implement"), named identifiers
- Re-prompt indicator: did the immediately following turn correct or redirect the AI
- Turns-to-resolution: exchanges before the conversation moved forward

Weak supervision labels derived directly from conversation outcomes: a prompt that leads to immediate re-correction is low quality; a prompt resolved in a single accepted response is higher quality.

Two-level scoring:

1. Per-prompt feature extraction + optional CodeBERT embedding (inference only)
2. Session-level aggregation: mean score + trajectory signal (did prompting quality improve or degrade over session)

### Input

Ordered sequence of prompt texts from the session, with associated response and follow-up turn metadata.

### Training Data

**WildChat (Allen Institute for AI)** — 1 million real human-ChatGPT interactions. Filtered to multi-turn coding sessions. Human prompt turns extracted with re-prompt rate and turns-to-resolution computed per conversation as weak supervision signal. Large scale allows selective filtering while retaining a substantial working dataset.

**Domain gap note:** WildChat captures ChatGPT conversations, not IDE-embedded sessions with execution context. Prompting patterns and quality signals are directionally consistent; in-domain session data replaces this proxy post-hackathon.

### Output

Score vector on semantic prompt dimensions (1–5 per dimension). Per-prompt scores passed to LLM judge to surface specific examples in the narrative.

---

## 6. Component 3 — Critical Review Detector

### What it does

Detects whether a candidate meaningfully reviewed and evaluated AI-generated code before accepting it. Operates on the diff between what the AI suggested and what the candidate actually committed.

### Architecture

**Normalized edit distance heuristic** between AI response code and candidate's final committed version of that code block. Thresholds calibrated against the distribution of post-acceptance edit rates observed in CUPS telemetry data.

| Edit distance (normalized) | Score |
| --- | --- |
| < 0.05 | 1 — verbatim paste |
| 0.05–0.15 | 2 — cosmetic changes only |
| 0.15–0.35 | 3 — moderate modification |
| 0.35–0.60 | 4 — substantial review |
| > 0.60 | 5 — significant independent work |

Session-level score = minimum of per-interaction scores (worst review moment is most diagnostic).

### Rationale for Heuristic Approach

CUPS provides empirical grounding for the threshold calibration, making this an informed heuristic rather than an arbitrary one. A trained classifier (e.g. fine-tuned CodeBERT on CodeReviewer diffs) is the production upgrade path; the heuristic is the hackathon-scoped version with equivalent directional validity.

### Output

Per-interaction review score (1–5). Session-level score passed to aggregation layer and LLM judge.

---

## 7. Aggregation Layer

### What it does

Combines output vectors from all three components into final per-dimension scores and an overall behavioral label.

### Architecture

**Weighted average** with weights derived from Component 1 XGBoost feature importances.

Rather than hardcoding dimension weights or training an MLP (which requires large labeled session data), the aggregation weights are informed by which behavioral signals are most predictive of the over-reliant / strategic distinction as learned from CUPS. This lets real data determine weighting rather than assumption.

Overall label (over-reliant / balanced / strategic) assigned by thresholding the weighted average score:

- < 2.5 → over-reliant
- 2.5–3.5 → balanced
- 3.5 → strategic

### Upgrade Path

Replace weighted average with a 2-layer MLP once real session data volume reaches 300+ labeled sessions. The MLP learns component interaction effects (e.g. high prompt quality partially compensates for low verification frequency) that a weighted average cannot capture.

---

## 8. LLM Judge — Claude

### Role

The LLM judge is not a trained model. It is a prompted Claude call that runs after all three components have scored the session. Its job is to produce the qualitative narrative that makes scores interpretable and actionable.

### Why it's still needed after training

Trained models produce numbers. Recruiters and candidates need explanations. "Prompt granularity: 2.8/5" is not actionable. "Candidate sent 7 prompts asking the AI to implement entire functions without specifying constraints or referencing the existing interface — this pattern suggests low strategic decomposition" is actionable.

The LLM judge also handles edge cases and session patterns the trained models haven't seen, providing a graceful fallback when component confidence is low.

### What it receives

- Component output scores + feature importance from Component 1
- Per-prompt scores and most diagnostic prompt examples from Component 2
- Per-interaction review scores and lowest-scoring diff from Component 3
- Aggregation layer final scores and overall label
- Session metadata (total time, phase breakdown, task description)

### What it produces

- 3–5 sentence behavioral summary
- Per-dimension narrative with specific session examples
- 2–3 concrete improvement suggestions
- Confidence flags where model certainty was low

### Prompt structure

The judge prompt includes: rubric definitions, score vectors with component breakdowns, 3–5 selected session excerpts (worst-scoring prompt, lowest-review diff, re-prompt sequence), and instructions to reason before scoring. Chain-of-thought is captured and shown to users as transparency into the scoring.

---

## 9. Datasets

| Dataset | Component | Usage |
| --- | --- | --- |
| CUPS (microsoft/coderec_programming_states) | Component 1 | Real Copilot telemetry — acceptance rates, deliberation time, post-acceptance edits, cognitive state annotations. Primary training data for structural behavioral classifier. |
| WildChat (Allen Institute for AI) | Component 2 | 1M real human-ChatGPT interactions, filtered to coding sessions. Weak supervision labels from re-prompt rate and turns-to-resolution. Primary training data for prompt quality scorer. |
| SWE-bench Lite | All / task environment | Problem library + grounding environment for synthetic trace generation |
| Synthetic sessions (generated) | Aggregation calibration | End-to-end labeled sessions — 3 behavioral profiles × SWE-bench tasks, counterfactual pairs. Used to validate aggregation layer thresholds. |

### Datasets Evaluated and Not Used

| Dataset | Reason Not Used |
| --- | --- |
| WildChat / LMSYS (as secondary) | WildChat sufficient as primary; LMSYS skewed toward model-comparison behavior |
| glaiveai/glaive-code-assistant | Likely synthetic/curated rather than real user behavior — undermines real-data claim |
| KIWI Dataset | Only ~1,200 turns — too small for training; retained as qualitative validation reference |
| MultiAIGCD | Output classification, not behavioral process data — wrong modality |
| HumanVsAI_CodeDataset | Output classification, not behavioral process data — wrong modality |
| A.S.E (AICGSecEval) | Security-focused benchmark — wrong domain entirely |

### Synthetic Data Generation Plan

- Sample 20–30 tasks from SWE-bench Lite
- For each task, generate 3 sessions via prompted Claude: over-reliant, balanced, strategic profiles
- Output: structured JSON session traces with behavioral label attached
- Include counterfactual pairs (same task, two profiles) for contrastive validation
- Target: 100–150 labeled sessions
- Use synthetic for aggregation threshold calibration and contrastive discrimination validation only — components train on real data

---

## 10. Instrumentation — Web IDE Event Logging

All features are extractable from standard Monaco editor events plus a thin logging layer. No exotic instrumentation required.

### Event Schema

```json
{
  "timestamp": "ISO8601",
  "session_id": "string",
  "actor": "user | ai",
  "event_type": "edit | execute | prompt | response | file_open | file_close | panel_focus",
  "content": "string",
  "metadata": {
    "file": "string",
    "exit_code": "int | null",
    "edit_delta": "unified diff string | null",
    "panel": "editor | chat | terminal | filetree | null",
    "cursor_position": "{ line, col } | null"
  }
}
```

### Event Sources by Metric

| Metric | Monaco / UI Event | Implementation Notes |
| --- | --- | --- |
| Verification frequency | Terminal execution events | Listen on terminal run button + keyboard shortcut |
| Re-prompt vs. debug ratio | Sequence of execute → prompt events | Post-session log analysis, no special listener |
| Time distribution | `panel_focus` change events + timestamps | Track active panel with focus/blur listeners |
| Orientation time | Time from session start to first `edit` event | Derived from log, no special listener |
| Iteration depth | `execute` events between `edit` sequences | Derived from log sequencing |
| Test lines written | `edit` event content analysis | Regex match on `def test_` patterns post-session |
| File open / dwell time | `file_open` + `file_close` events | Tab open/close in file explorer |
| Prompt granularity | `prompt` event content | String in hand — length, code block presence, function name references |
| Prompt context quality | `prompt` event content | Passed to Component 2 for classification |
| AI output modification | `response` content vs. subsequent `edit` content | Diff AI response against next committed version |
| Problem decomposition | First 3–5 `prompt` events | Passed to Component 2, session-start slice |

### Debouncing

Monaco fires `onDidChangeModelContent` on every keystroke. Store edit events debounced at 2-second pause intervals — any edit activity within a 2s window is collapsed into a single edit event with the net diff. Prevents log bloat and noise in diff history.

### Data Volume Estimate

~500–2,000 events per 60-minute session after debouncing. Well within reasonable storage and processing limits.

---

## 11. Feature-to-Component Mapping (Full)

| Feature | Instrumentation Complexity | Component | Training Data |
| --- | --- | --- | --- |
| Verification frequency | Low — terminal listener | Component 1 | CUPS + synthetic |
| Re-prompt vs. debug ratio | Low — log sequencing | Component 1 | CUPS + synthetic |
| Time distribution | Low — panel focus tracking | Component 1 | CUPS + synthetic |
| Orientation time | Low — derived from log | Component 1 | CUPS + synthetic |
| Iteration depth | Low — derived from log | Component 1 | CUPS + synthetic |
| Test coverage | Low — post-session static analysis | Component 1 | CUPS + synthetic |
| File open + dwell time | Medium — tab event tracking | Component 1 | CUPS + synthetic |
| Acceptance rate / deliberation | Mapped from CUPS signal | Component 1 | CUPS |
| Prompt granularity | Low — prompt string parsing | Component 2 | WildChat coding subset |
| Prompt context quality | Low — prompt text in hand | Component 2 | WildChat coding subset |
| Problem decomposition | Low — early prompt slice | Component 2 | WildChat coding subset |
| Conceptual understanding | Low — prompt text in hand | Component 2 | WildChat coding subset |
| AI output modification rate | Medium — diff computation | Component 3 | CUPS-calibrated heuristic |
| Critical review quality | Medium — normalized edit distance | Component 3 | CUPS-calibrated heuristic |

---

## 12. Training Plan

### Prerequisites

- Session logging pipeline instrumented and tested
- SWE-bench Lite environments running for synthetic generation
- WildChat coding subset filtered and prompt features extracted
- CUPS telemetry preprocessed and proxy labels derived

### Order of Operations

**Step 1 — Data preprocessing (runs async, start immediately)**

- Filter WildChat to multi-turn coding conversations; extract prompt turns with re-prompt rate labels
- Preprocess CUPS telemetry; derive behavioral proxy labels from acceptance/deliberation/edit-rate signals
- Generate 100–150 synthetic sessions via Claude API for aggregation calibration

**Step 2 — Component 1 (highest infrastructure priority)**
Train XGBoost on CUPS-derived features and proxy labels. Fastest to train. Unblocks feature importance weights needed for aggregation layer.

**Step 3 — Component 2 (highest ML priority)**
Train LightGBM on WildChat prompt features. Optionally add CodeBERT embeddings (inference only — no fine-tuning) as additional features. Most novel and differentiated component. Do prompt quality EDA first to validate label quality before training.

**Step 4 — Component 3**
Implement normalized edit distance heuristic. Calibrate thresholds against CUPS post-acceptance edit rate distribution. Validate that threshold buckets produce meaningful separation on sample sessions.

**Step 5 — Aggregation layer**
Set weighted average weights from Component 1 feature importances. Validate on synthetic session contrastive pairs — model must rank strategic above over-reliant on same task.

**Step 6 — LLM judge prompt engineering**
Calibrate judge prompt against 10–15 complete sessions (real sessions captured during hackathon preferred). Adjust rubric definitions, example selection logic, and output format until narrative quality is consistently useful.

### Hackathon Fallback Priority

If time is short, cut in this order:

1. Drop CodeBERT embeddings from Component 2 — run LightGBM on engineered features only
2. Simplify Component 1 to rule-based heuristics with CUPS-informed thresholds
3. Run Component 2 alone + LLM judge — this is the minimum viable behavioral model

---

## 13. Validation

| Check | Method | Target |
| --- | --- | --- |
| Behavioral score consistency | Human rater study — 3 raters score 20 sessions, compare to model | Cohen's kappa > 0.70 |
| Component 1 label accuracy | Held-out CUPS sessions | Accuracy > 0.75 on 3-class label |
| Component 2 prompt classification | Held-out WildChat conversations | F1 > 0.72 on quality categories |
| Component 3 review detection | Manual review of 20 session diffs vs. heuristic scores | Directional agreement > 80% |
| Aggregation label accuracy | Held-out synthetic sessions | Accuracy > 0.80 on 3-class label |
| LLM judge narrative quality | Blind evaluation of 10 reports | > 4/5 usefulness rating |
| Contrastive discrimination | Same task, two profiles — model must rank them correctly | 100% on held-out counterfactual pairs |

---

## 14. Open Questions

- **Domain gap bridging** — CUPS (Copilot inline) and WildChat (ChatGPT chat) are proxies for in-IDE chat-based sessions. Quantifying transfer gap requires running real sessions and comparing feature distributions to training data.
- **Rubric dimension weights in aggregation** — currently derived from Component 1 feature importances. Needs calibration against real hiring outcomes once enterprise customers are running sessions.
- **Weak supervision label quality in WildChat** — re-prompt rate as a proxy for prompt quality is noisy. Prompts can fail for reasons unrelated to quality (hard task, ambiguous spec). Needs empirical validation against human-rated prompt samples.
- **Re-prompt window definition** — currently 60s after a failed execution counts as re-prompting rather than debugging. This threshold needs empirical validation against real sessions.
- **Legal defensibility of behavioral scoring in hiring decisions** — EU AI Act and EEOC guidance both apply. Model explainability (LLM judge chain-of-thought) partially addresses this but needs legal review before enterprise deployment.