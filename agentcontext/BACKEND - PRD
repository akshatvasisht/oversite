# PRD - BACKEND

---

# AI-Assisted Coding Interview Assessment Platform

*Session logging · API design · Behavioral scoring pipeline · Analytics*

| Field | Detail |
| --- | --- |
| Document type | Backend PRD — Hackathon MVP scope |
| Stack | Python 3.11 · Flask · SQLite · SQLAlchemy |
| Scope | Session logging, AI proxy, behavioral scoring, analytics API |
| Target runtime | Single-server local or simple VPS deployment |
| Last updated | February 2026 |
| Version | v4 — Cursor-style diff editor + updated model architecture |

---

## Changelog from v3

This version merges two parallel sets of changes: the **Cursor-style diff editor** introduced in v3, and the **updated model architecture** from the revised Model PRD.

**From the updated Model PRD:**

- **LLM changed to Gemini Flash** — `GEMINI_API_KEY` replaces `ANTHROPIC_API_KEY` for the AI assistant proxy. The LLM judge still uses Claude (separate key). Both keys live only on the backend.
- **Component 1 trains on CUPS** (microsoft/coderec_programming_states) — real Copilot telemetry. Three new features added to the feature vector: `acceptance_rate` (proxy: chunk acceptance rate from `chunk_decisions`), `deliberation_time` (proxy: mean `time_on_chunk_ms`), `post_acceptance_edit_rate` (proxy: modification rate on accepted chunks).
- **Component 2 uses LightGBM + optional CodeBERT embeddings** (inference only, no fine-tuning). Trains on WildChat coding subset. Architecture changed from fine-tuned CodeBERT to gradient boosted classifier on engineered features.
- **Component 3 is now a heuristic** — normalized edit distance with thresholds calibrated from CUPS post-acceptance edit rate distribution. Not a trained model. No model artifact needed.
- **Aggregation layer is a weighted average** — weights derived from Component 1 XGBoost feature importances. Not an MLP. Simpler and unblocks scoring before large labeled session datasets exist.
- **`session_scores` table fully defined** — stores all component outputs, overall label, weighted score, LLM narrative, and judge chain-of-thought.
- **Scoring pipeline triggers at session end** — Components 1–3 + aggregation run synchronously on `POST /session/end`. LLM judge runs async and updates `llm_narrative` when complete.
- **Model artifacts loaded via `joblib`** from `models/` directory. Graceful fallback to rule-based heuristics if artifacts absent.
- **`SCORING_FALLBACK_MODE`** env var added — forces heuristic mode even if artifacts present. Useful for testing.
- **`POST /analytics/session/:id/score`** — manual re-score endpoint added.

**Carried forward from v3 (Cursor diff model):**

- `ai_suggestions` + `chunk_decisions` tables
- `suggestion_shown`, `chunk_accepted`, `chunk_rejected`, `chunk_modified` event types
- `POST /suggestions` and `POST /suggestions/:id/chunks/:idx/decide` endpoints
- Explicit modification rate from `proposed_code` vs `final_code` — no snapshot diffing needed
- Dual-write pattern for all events
- `edit_delta` on all editor snapshots
- 2-second debounce

---

## 1. Overview & Design Philosophy

The backend serves three distinct roles: the **session environment** (managing files and state), the **observability layer** (logging every meaningful candidate-AI interaction), and the **analytics engine** (running the behavioral scoring pipeline and serving results).

The backend has two equally important consumers:

1. **The analytics API** — serves the real-time dashboard and candidate report
2. **The model pipeline** — consumes the session trace for behavioral scoring via trained model artifacts

The **dual-write pattern** ensures both consumers are satisfied without requiring joins across multiple tables at scoring time. Every loggable action writes to a structured table (for API efficiency) and the unified `events` table (for model consumption) in a single atomic SQLite transaction.

> **Hackathon constraint:** Single-server, SQLite-backed. Scalability and production hardening explicitly out of scope.
> 

> **LLM:** `GEMINI_API_KEY` powers both the in-IDE AI assistant (Gemini Flash) and the LLM judge. Lives only on the backend. The frontend has no access to it.
> 

---

## 2. System Architecture

- **Frontend (React + Monaco)** — renders diff overlays using Monaco decorations and overlay widgets. Sends all events to the backend. Never calls any LLM API directly.
- **Flask API Layer** — proxies Gemini calls (assistant and judge), parses AI responses into structured hunk lists, logs all events, runs scoring pipeline, serves analytics.
- **SQLite Database** — unified `events` log plus structured tables. Single file, zero config.
- **Scoring Engine** (`scoring.py`) — loads `joblib` model artifacts at startup, runs Components 1–3 + aggregation, calls Claude judge. Falls back to heuristics if artifacts absent.

### Cursor-style interaction flow

```
Candidate types prompt
        │
        ▼
POST /ai/chat  →  Gemini Flash API
        │  Response contains explanation text + proposed code changes
        │  Backend logs prompt event + response event
        │
        ▼
POST /suggestions
        │  Backend diffs original_content vs proposed_content
        │  Parses into ordered hunk list
        │  Stores ai_suggestions row
        │  Returns { suggestion_id, hunks: [{ index, original_code, proposed_code, start_line }] }
        │
        ▼
Frontend renders Monaco diff overlay
  (green = additions, red = removals, per hunk)
        │
  Per hunk, candidate decides:
        ├── Accept  →  POST /suggestions/:id/chunks/:idx/decide { decision: "accepted", final_code: proposed_code }
        ├── Reject  →  POST /suggestions/:id/chunks/:idx/decide { decision: "rejected", final_code: original_code }
        └── Modify  →  edits green text, then accepts
                    →  POST /suggestions/:id/chunks/:idx/decide { decision: "modified", final_code: "<edited>" }
        │
        ▼
POST /session/end
        │  Triggers synchronous scoring pipeline:
        │    Component 1 (XGBoost or heuristic fallback)
        │    Component 2 (LightGBM or heuristic fallback)
        │    Component 3 (edit distance heuristic, always)
        │    Aggregation layer (weighted average)
        │  Writes session_scores row
        │  Queues async Claude judge pass → updates llm_narrative when complete
```

### 2.1 Technology Choices

| Component | Choice | Rationale |
| --- | --- | --- |
| Web framework | Flask | Minimal, fast to iterate on. |
| Database | SQLite via SQLAlchemy | Zero setup, single file, sufficient for hundreds of sessions. |
| AI assistant LLM | Gemini Flash (free tier) | Cost-effective for hackathon. Google Generative AI Python SDK. |
| LLM judge | Gemini (same client) | Same API key and client. Judge uses a separate system prompt and model call. |
| Diff computation | Python `difflib` | Built-in. Unified diff for `edit_delta`; SequenceMatcher for modification rate. |
| Scoring models | `joblib` (XGBoost + LightGBM) | Standard scikit-learn-compatible serialization. Fast load at startup. |
| CORS | Flask-CORS | Required for cross-port dev setup. |
| Env config | python-dotenv | API keys kept out of source. |

---

## 3. Database Schema

Eight tables total. Compared to v3: `session_scores` table is now fully defined (was a stub). All other tables unchanged from v3.

### 3.1 `events` — Unified model-facing log

The canonical event log. Every loggable action in the system writes a row here. Model pipeline reads `SELECT * FROM events WHERE session_id = ? ORDER BY timestamp ASC` — no joins required.

```json
{
  "timestamp": "ISO8601",
  "session_id": "string",
  "actor": "user | ai",
  "event_type": "edit | execute | prompt | response | file_open | file_close | panel_focus | suggestion_shown | chunk_accepted | chunk_rejected | chunk_modified",
  "content": "string",
  "metadata": {
    "file": "string | null",
    "exit_code": "int | null",
    "edit_delta": "unified diff string | null",
    "panel": "editor | chat | terminal | filetree | null",
    "cursor_position": "{ line, col } | null",
    "interaction_id": "string | null",
    "suggestion_id": "string | null",
    "chunk_index": "int | null",
    "decision": "accepted | rejected | modified | null",
    "time_on_chunk_ms": "int | null",
    "phase": "string | null"
  }
}
```

| Column | Type | Notes |
| --- | --- | --- |
| `event_id` | TEXT PK | UUID. |
| `session_id` | TEXT FK → sessions |  |
| `timestamp` | DATETIME NOT NULL | Server time. Never trust client timestamps. |
| `actor` | TEXT NOT NULL | `user` or `ai`. |
| `event_type` | TEXT NOT NULL | One of the 11 types above. |
| `content` | TEXT | Primary content — see mapping below. |
| `metadata` | TEXT | JSON blob. Null fields stored as JSON null. |

**`content` field by event type:**

| `event_type` | `content` |
| --- | --- |
| `edit` | Empty — change is in `metadata.edit_delta` |
| `execute` | Command string, e.g. `python solution.py` |
| `prompt` | Full prompt text |
| `response` | Full AI response text |
| `file_open` | Filename |
| `file_close` | Filename |
| `panel_focus` | Panel: `editor`, `chat`, `terminal`, `filetree` |
| `suggestion_shown` | `suggestion_id` |
| `chunk_accepted` | `proposed_code` of the accepted hunk |
| `chunk_rejected` | `proposed_code` of the rejected hunk |
| `chunk_modified` | `final_code` — what the candidate changed it to |

### 3.2 `sessions`

| Column | Type | Notes |
| --- | --- | --- |
| `session_id` | TEXT PK | UUID. Passed as `X-Session-ID` header on all requests. |
| `username` | TEXT NOT NULL | Candidate-supplied name. No auth. |
| `project_name` | TEXT | Task/problem name. |
| `started_at` | DATETIME |  |
| `ended_at` | DATETIME | NULL if abandoned. |
| `phase` | TEXT | `orientation` | `implementation` | `verification`. |

### 3.3 `files`

| Column | Type | Notes |
| --- | --- | --- |
| `file_id` | TEXT PK | UUID. |
| `session_id` | TEXT FK → sessions |  |
| `filename` | TEXT NOT NULL | Relative path, e.g. `src/utils.py`. |
| `language` | TEXT | Inferred from extension. |
| `created_at` | DATETIME |  |
| `initial_content` | TEXT | Baseline for diff chains. |

### 3.4 `ai_interactions`

One row per prompt/response pair. Parent of `ai_suggestions`.

| Column | Type | Notes |
| --- | --- | --- |
| `interaction_id` | TEXT PK | UUID. Referenced in `events.metadata.interaction_id`. |
| `session_id` | TEXT FK → sessions |  |
| `file_id` | TEXT FK → files | Active file when prompt was sent. Nullable. |
| `prompt` | TEXT NOT NULL | Full prompt text. |
| `response` | TEXT NOT NULL | Full Gemini response text. |
| `model` | TEXT | Model identifier, e.g. `gemini-1.5-flash`. |
| `prompt_tokens` | INTEGER |  |
| `shown_at` | DATETIME | T0 for timing metrics. |
| `phase` | TEXT | Session phase at time of prompt. |

### 3.5 `ai_suggestions`

One row per AI response that proposes file changes. A single `ai_interaction` may produce zero or one `ai_suggestion` (zero if the response was explanatory only, with no proposed code changes).

| Column | Type | Notes |
| --- | --- | --- |
| `suggestion_id` | TEXT PK | UUID. |
| `interaction_id` | TEXT FK → ai_interactions | The prompt/response that generated this suggestion. |
| `session_id` | TEXT FK → sessions | Denormalized. |
| `file_id` | TEXT FK → files | Which file this suggestion modifies. |
| `original_content` | TEXT NOT NULL | Full file content before the suggestion. Snapshot at creation time. |
| `proposed_content` | TEXT NOT NULL | Full file as proposed by the AI. Diff computed from original → proposed. |
| `hunks_count` | INTEGER | Number of hunks. Pre-computed. |
| `shown_at` | DATETIME | When the diff overlay was rendered. T0 for `time_on_chunk_ms`. |
| `resolved_at` | DATETIME | When all hunks were decided. NULL until complete. |
| `all_accepted` | BOOLEAN | True if every hunk was accepted without modification. Quick filter for passive acceptance. |
| `any_modified` | BOOLEAN | True if any hunk was modified before accepting. Positive critical review signal. |

### 3.6 `chunk_decisions`

One row per hunk per decision. The most analytically important table. Passive acceptance vs. critical review is most directly observable here.

| Column | Type | Notes |
| --- | --- | --- |
| `decision_id` | TEXT PK | UUID. |
| `suggestion_id` | TEXT FK → ai_suggestions |  |
| `session_id` | TEXT FK → sessions | Denormalized. |
| `file_id` | TEXT FK → files |  |
| `chunk_index` | INTEGER | 0-indexed position in the suggestion's hunk list. |
| `original_code` | TEXT NOT NULL | Code as it existed before the suggestion for this hunk's line range. |
| `proposed_code` | TEXT NOT NULL | Exactly what the AI proposed. |
| `final_code` | TEXT NOT NULL | What ended up in the file. Equals `proposed_code` if accepted, `original_code` if rejected, or candidate-edited version if modified. |
| `decision` | TEXT NOT NULL | `accepted` | `rejected` | `modified`. |
| `decided_at` | DATETIME | When the candidate clicked Accept/Reject. |
| `time_on_chunk_ms` | INTEGER | Time spent on this hunk before deciding. See timing rules in Section 4.4. |
| `chunk_start_line` | INTEGER | Line number in the file where this hunk begins. |
| `char_count_proposed` | INTEGER | `len(proposed_code)`. Used to normalize `time_on_chunk_ms`. |

### 3.7 `editor_events`

Debounced edit snapshots. Debounce window is **2 seconds**.

| Column | Type | Notes |
| --- | --- | --- |
| `event_id` | TEXT PK | UUID. |
| `session_id` | TEXT FK → sessions |  |
| `file_id` | TEXT FK → files |  |
| `trigger` | TEXT | `debounce` | `post_suggestion` | `save` | `phase_change`. |
| `content` | TEXT NOT NULL | Full file content snapshot. |
| `edit_delta` | TEXT | Unified diff vs. previous snapshot. Computed on backend. Used by Component 3 for manual edits outside suggestion flow. |
| `suggestion_id` | TEXT FK → ai_suggestions NULLABLE | Set when `trigger = post_suggestion`. |
| `cursor_line` | INTEGER |  |
| `cursor_col` | INTEGER |  |
| `timestamp` | DATETIME |  |
| `char_count` | INTEGER |  |

### 3.8 `session_scores` ⭐ Fully defined in v4

Stores the full output of the scoring pipeline per session. Written once at session end. Cached so analytics endpoints never re-run inference.

| Column | Type | Notes |
| --- | --- | --- |
| `score_id` | TEXT PK | UUID. |
| `session_id` | TEXT FK → sessions |  |
| `computed_at` | DATETIME | When scoring pipeline ran. |
| `structural_scores` | TEXT | JSON blob — Component 1 output: `{ verification_frequency: 3.2, reprompt_ratio: 2.1, time_distribution: 4.0, iteration_depth: 3.5, test_coverage: 2.0, codebase_exploration: 3.8, acceptance_rate: 2.5, deliberation_time: 3.1, post_acceptance_edit_rate: 4.2 }`. Score 1–5 per dimension. |
| `prompt_quality_scores` | TEXT | JSON blob — Component 2 output: `{ granularity: 3.0, context_quality: 2.5, problem_decomposition: 3.5, conceptual_understanding: 2.8 }`. Score 1–5 per dimension. |
| `review_scores` | TEXT | JSON blob — Component 3 output: `{ per_interaction: [{ interaction_id, score }], session_level: 2.3 }`. Score 1–5. |
| `overall_label` | TEXT | `over_reliant` | `balanced` | `strategic`. |
| `weighted_score` | REAL | Aggregated 0–5 score from weighted average. |
| `feature_importances` | TEXT | JSON blob — Component 1 XGBoost feature importance weights. Used in aggregation and passed to LLM judge. |
| `fallback_components` | TEXT | JSON array of component names that ran in heuristic fallback mode, e.g. `["component1", "component2"]`. Empty array if all models loaded. |
| `llm_narrative` | TEXT | Full qualitative report from Claude judge. NULL until async judge pass completes. |
| `judge_chain_of_thought` | TEXT | Claude judge reasoning trace. Stored for transparency. NULL until async pass completes. |

> **Caching note:** `computed_at` allows the frontend to show when scores were last updated. Re-scoring via `POST /analytics/session/:id/score` overwrites the existing row rather than creating a new one, to keep one authoritative score per session.
> 

---

## 4. API Endpoints

All endpoints prefixed `/api/v1`. Session context via `X-Session-ID` header. All responses JSON.

### 4.1 Session Management

### `POST /session/start`

| Field | Detail |
| --- | --- |
| Request body | `{ "username": "Alice", "project_name": "E-commerce Cart Bug" }` |
| Response | `{ "session_id": "<uuid>", "started_at": "<iso datetime>" }` |
| Side effects | Inserts into `sessions` with `phase = orientation`. |

### `POST /session/end`

| Field | Detail |
| --- | --- |
| Request body | `{ "final_phase": "verification" }` |
| Response | `{ "session_id": "<uuid>", "ended_at": "<iso datetime>", "duration_seconds": 3420 }` |
| Side effects | 1) Sets `ended_at`. 2) **Triggers synchronous scoring pipeline** — Components 1, 2, 3, aggregation layer. 3) Writes `session_scores` row. 4) Queues async Claude judge pass — updates `llm_narrative` and `judge_chain_of_thought` when complete. Any unresolved suggestion hunks are auto-rejected before scoring runs. |

### `PATCH /session/phase`

| Field | Detail |
| --- | --- |
| Request body | `{ "phase": "implementation" }` |
| Response | `{ "phase": "implementation", "updated_at": "<iso datetime>" }` |
| Side effects | Updates `sessions.phase`. Dual-writes `panel_focus` event. |

### `GET /session/:session_id/trace`

| Field | Detail |
| --- | --- |
| Purpose | Full ordered event sequence for model pipeline consumption. |
| Response | `{ "session_id": "<uuid>", "events": [ ...ordered by timestamp ASC... ] }` |
| Notes | `metadata` parsed from JSON string to object. Single table scan, no joins. |

### 4.2 File Management

### `POST /files`

| Field | Detail |
| --- | --- |
| Request body | `{ "filename": "src/cart.py", "initial_content": "<full text>" }` |
| Response | `{ "file_id": "<uuid>", "filename": "src/cart.py" }` |
| Side effects | Inserts into `files`. Dual-writes `file_open` event. Writes initial `editor_events` snapshot with `edit_delta = null`. |

### `POST /files/:file_id/save`

| Field | Detail |
| --- | --- |
| Request body | `{ "content": "<full text>" }` |
| Response | `{ "event_id": "<uuid>", "timestamp": "<iso datetime>" }` |
| Side effects | Computes `edit_delta`, inserts into `editor_events` with `trigger = save`. Dual-writes `edit` event. |

### `POST /events/file`

| Field | Detail |
| --- | --- |
| Request body | `{ "filename": "src/utils.py", "action": "open" | "close" }` |
| Response | `{ "event_id": "<uuid>", "timestamp": "<iso datetime>" }` |
| Side effects | Dual-writes `file_open` or `file_close` event to `events`. |

### 4.3 AI Proxy

### `POST /ai/chat`

| Field | Detail |
| --- | --- |
| Purpose | Proxy candidate prompt to Gemini Flash. Log interaction. Return response. |
| Request body | `{ "prompt": "Refactor this to handle edge cases", "file_id": "<uuid>", "conversation_history": [...] }` |
| Response | `{ "interaction_id": "<uuid>", "response": "<full Gemini response>", "has_code_changes": true, "shown_at": "<iso datetime>" }` |
| Side effects | 1) Calls Gemini Flash API. 2) Inserts into `ai_interactions`. 3) Dual-writes `prompt` (actor=user) and `response` (actor=ai) events. If API fails, neither row is written. 4) `has_code_changes` hints to frontend whether to call `POST /suggestions`. |

### 4.4 Suggestion Management

### `POST /suggestions`

Core new endpoint. Called after receiving an AI response with code changes.

| Field | Detail |
| --- | --- |
| Purpose | Register AI suggestion, parse into hunks, return structured hunk list for frontend to render as diff overlay. |
| Request body | `{ "interaction_id": "<uuid>", "file_id": "<uuid>", "original_content": "<current file>", "proposed_content": "<AI proposed file>" }` |
| Response | `{ "suggestion_id": "<uuid>", "shown_at": "<iso datetime>", "hunks": [ { "index": 0, "original_code": "...", "proposed_code": "...", "start_line": 14, "end_line": 22, "char_count_proposed": 180 } ] }` |
| Side effects | 1) Runs `difflib.unified_diff(original_content, proposed_content)`, parses hunks. 2) Inserts into `ai_suggestions`. 3) Dual-writes `suggestion_shown` event. 4) Writes `editor_events` snapshot with `trigger = post_suggestion` as pre-decision baseline. |
| Validation | Returns 400 if `proposed_content == original_content` — no diff means no suggestion. |

### `POST /suggestions/:suggestion_id/chunks/:chunk_index/decide`

Called once per hunk as the candidate makes each decision.

| Field | Detail |
| --- | --- |
| Purpose | Record accept/reject/modify decision on a specific hunk with timing. |
| Request body | `{ "decision": "accepted" | "rejected" | "modified", "final_code": "<what ended up in file>", "time_on_chunk_ms": 8400 }` |
| Response | `{ "decision_id": "<uuid>", "decided_at": "<iso datetime>" }` |
| Side effects | 1) Inserts into `chunk_decisions`. 2) Dual-writes `chunk_accepted`, `chunk_rejected`, or `chunk_modified` event. 3) If last hunk, sets `ai_suggestions.resolved_at`, `all_accepted`, `any_modified`. All in one transaction. |
| `final_code` discipline | Frontend must always send actual code: `accepted → proposed_code`, `rejected → original_code`, `modified → edited version`. |
| Timing rules | `time_on_chunk_ms` for chunk 0 = `decided_at − suggestion.shown_at`. For chunk N > 0 = `decided_at − previous_chunk.decided_at`. Computed client-side, sent with request. |
| Validation | Returns 400 if `chunk_index` invalid. Returns 409 if chunk already decided. |

### `GET /suggestions/:suggestion_id`

| Field | Detail |
| --- | --- |
| Purpose | Full suggestion state including all chunk decisions. Used by analytics and dashboard. |
| Response | `ai_suggestions` row + array of `chunk_decisions` ordered by `chunk_index`. |

### 4.5 Interaction Tracking

### `POST /events/editor`

| Field | Detail |
| --- | --- |
| Request body | `{ "file_id": "<uuid>", "content": "<full text>", "trigger": "debounce", "suggestion_id": null, "cursor_line": 14, "cursor_col": 22 }` |
| Response | `{ "event_id": "<uuid>", "timestamp": "<iso datetime>", "edit_delta": "<unified diff>" }` |
| Side effects | Retrieves previous snapshot, computes `edit_delta`, inserts into `editor_events`, dual-writes `edit` event. |
| Debounce | **2 seconds.** Frontend must not send more frequently. |

### `POST /events/execute`

| Field | Detail |
| --- | --- |
| Request body | `{ "command": "python solution.py", "exit_code": 1, "output": "<stdout/stderr>" }` |
| Response | `{ "event_id": "<uuid>", "timestamp": "<iso datetime>" }` |
| Side effects | Dual-writes `execute` event with `metadata.exit_code`. |

### `POST /events/panel`

| Field | Detail |
| --- | --- |
| Request body | `{ "panel": "editor" | "chat" | "terminal" | "filetree" }` |
| Response | `{ "event_id": "<uuid>", "timestamp": "<iso datetime>" }` |
| Side effects | Dual-writes `panel_focus` event. Fire on every focus change — no debounce. |

### 4.6 Analytics

### `GET /analytics/session/:session_id`

Returns raw metrics (always computed live) plus cached model scores from `session_scores` if available.

**Raw metrics (always live):**

| Metric | Source | Computation |
| --- | --- | --- |
| `chunk_acceptance_rate` | `chunk_decisions` | `decision = accepted` / total chunks |
| `chunk_rejection_rate` | `chunk_decisions` | `decision = rejected` / total chunks |
| `chunk_modification_rate` | `chunk_decisions` | `decision = modified` / total chunks |
| `passive_acceptance_rate` | `chunk_decisions` | `accepted AND time_on_chunk_ms < 3000` / total acceptances |
| `time_on_chunk_avg_ms` | `chunk_decisions` | Mean `time_on_chunk_ms` |
| `time_on_chunk_distribution` | `chunk_decisions` | Bucketed per Section 5.3. CUPS-calibrated thresholds. |
| `modification_depth_avg` | `chunk_decisions` WHERE `decision = modified` | Mean `1 − SequenceMatcher(proposed_code, final_code).ratio()` |
| `verification_frequency` | `events` | `execute` count / session duration minutes |
| `reprompt_ratio` | `events` | Failed `execute` → `prompt` within 60s / total failures |
| `time_by_panel` | `events` | Duration between consecutive `panel_focus` events grouped by panel |
| `orientation_duration_s` | `events` | Session start → first `edit` event |
| `iteration_depth` | `events` | Count of `edit → execute` cycles |
| `file_dwell_times` | `events` | Duration between `file_open`/`file_close` pairs per file |
| `prompt_count_by_phase` | `ai_interactions` | COUNT grouped by `phase` |
| `followup_question_rate` | `ai_interactions` | % followed by another within 120s |

**Model scores (from `session_scores` cache when available):**

| Field | Source |
| --- | --- |
| `overall_label` | `over_reliant` | `balanced` | `strategic` |
| `weighted_score` | Aggregated 0–5 |
| `structural_scores` | Component 1 — 1–5 per structural dimension |
| `prompt_quality_scores` | Component 2 — 1–5 per prompt quality dimension |
| `review_scores` | Component 3 — per-interaction and session-level |
| `llm_narrative` | Claude judge qualitative report. NULL until async pass completes. |
| `judge_chain_of_thought` | Claude judge reasoning. NULL until async pass completes. |
| `fallback_components` | Which components ran in heuristic mode. |
| `scores_available` | `true` if `session_scores` row exists, `false` if session still in progress. |

### `GET /analytics/overview`

Aggregate across all completed sessions. Filter: `?project_name=X`, `?min_duration=N`, `?completed_only=true`. Returns per-session summary rows (including `overall_label`, `weighted_score`) plus cohort-level averages.

### `POST /analytics/session/:session_id/score`

| Field | Detail |
| --- | --- |
| Purpose | Manually trigger re-scoring. Useful during development and after model artifact updates. |
| Response | `{ "score_id": "<uuid>", "computed_at": "<iso datetime>", "overall_label": "balanced", "fallback_components": [] }` |
| Side effects | Runs Components 1–3 + aggregation synchronously. Overwrites existing `session_scores` row. Queues async Claude judge pass. |

---

## 5. Behavioral Scoring Engine

`scoring.py` loads trained model artifacts at startup via `joblib`. Runs synchronously at session end. Falls back to rule-based heuristics per component if artifacts are absent.

### 5.1 Model Loading

```python
import joblib, os

def load_models():
    models = {}
    c1_path = "models/component1_xgboost.joblib"
    c2_path = "models/component2_lgbm.joblib"
    models["component1"] = joblib.load(c1_path) if os.path.exists(c1_path) else None
    models["component2"] = joblib.load(c2_path) if os.path.exists(c2_path) else None
    # Component 3 is always a heuristic — no artifact needed
    return models
```

If a model is `None`, that component runs in fallback heuristic mode and `fallback_components` is set accordingly in `session_scores`.

### 5.2 Component 1 — Structural Behavior Classifier (XGBoost)

Extracts feature vector from SQLite and runs XGBoost inference. Features are direct mappings from the v4 event schema to the CUPS-derived feature set:

| CUPS feature | v4 backend proxy | Source |
| --- | --- | --- |
| Verification frequency | `execute` count / session minutes | `events` |
| Re-prompt ratio | Failed `execute` → `prompt` within 60s / failures | `events` |
| Time in editor | Duration from `panel_focus = editor` events | `events` |
| Time in chat | Duration from `panel_focus = chat` events | `events` |
| Time in file tree | Duration from `panel_focus = filetree` events | `events` |
| Orientation time | Session start → first `edit` | `events` |
| Iteration count | `edit → execute` cycle count | `events` |
| Test lines written | `def test_` matches in `edit_delta` insertions | `editor_events` |
| File open count | Unique `file_open` events before first `edit` | `events` |
| Dwell time per file | Mean `file_open` → `file_close` duration | `events` |
| Prompt count | Total `prompt` events | `events` |
| Prompt rate | Prompt count / session minutes | `events` |
| **Acceptance rate** | `chunk_decisions WHERE decision = accepted` / total chunks | `chunk_decisions` |
| **Deliberation time** | Mean `time_on_chunk_ms` | `chunk_decisions` |
| **Post-acceptance edit rate** | Modification depth on accepted+modified chunks | `chunk_decisions` |

**Fallback (no artifact):** Rule-based thresholds informed by CUPS telemetry distributions, documented inline in `scoring.py`.

Output: 1–5 per structural dimension + XGBoost feature importance weights passed to aggregation layer.

### 5.3 Component 2 — Prompt Quality Scorer (LightGBM)

Retrieves all `prompt` events ordered by `shown_at`. Extracts engineered features per prompt:

- Length and structure
- Code context presence (backtick/code block detection, function/file name references)
- Specificity signals (constraint language, scoped verbs, named identifiers)
- Re-prompt indicator (did the next turn correct/redirect the AI)
- Turns-to-resolution (exchanges before conversation moved forward)

Optionally augments with CodeBERT embeddings if `models/component2_codebert_embeddings/` directory is present (inference only — no fine-tuning required). LightGBM runs on engineered features ± embeddings.

**Fallback (no artifact):** Heuristic thresholds on prompt length, code block presence, and function name reference count.

Output: 1–5 per prompt quality dimension. Per-prompt scores stored for LLM judge diagnostic example selection.

### 5.4 Component 3 — Critical Review Detector (Heuristic)

No model artifact. Always runs as a calibrated heuristic. For each `chunk_decision`:

```python
from difflib import SequenceMatcher

def review_score(proposed_code, final_code, decision):
    if decision == "rejected":
        return 5  # Rejected = highest critical review signal
    similarity = SequenceMatcher(None, proposed_code, final_code).ratio()
    modification_rate = 1 - similarity
    # Thresholds calibrated from CUPS post-acceptance edit rate distribution
    if modification_rate < 0.05:   return 1  # Verbatim paste
    if modification_rate < 0.15:   return 2  # Cosmetic changes
    if modification_rate < 0.35:   return 3  # Moderate modification
    if modification_rate < 0.60:   return 4  # Substantial review
    return 5                                  # Significant independent work
```

Session-level score = minimum of per-chunk scores (worst review moment is most diagnostic).

> **Note on rejected chunks:** A `decision = rejected` is a strong critical review signal — the candidate evaluated the proposed code and decided it was wrong. Scoring this as 5 reflects that deliberate rejection requires more critical thinking than passive acceptance. The session-level minimum therefore only penalizes sessions where every interaction was passively accepted without modification.
> 

### 5.5 Time-on-Chunk Buckets

Thresholds calibrated from CUPS deliberation time distributions.

| Bucket | Range | Interpretation |
| --- | --- | --- |
| Instant | < 3 seconds | Almost certainly did not read. Strong passive acceptance signal. |
| Quick scan | 3 – 15 seconds | Possible skim. Moderate signal depending on chunk length. |
| Read | 15 – 60 seconds | Plausible the hunk was read. Neutral. |
| Studied | > 60 seconds | Deliberately reviewed before deciding. Positive signal. |

Normalized: `time_on_chunk_ms / max(1, char_count_proposed / 50)` — seconds per 50 chars.

### 5.6 Aggregation Layer (Weighted Average)

Combines Component 1, 2, and 3 outputs into a final score and overall label. Weights derived from Component 1 XGBoost feature importances — letting the data determine which behavioral signals are most diagnostic rather than hardcoding assumptions.

```python
def aggregate_scores(c1_scores, c2_scores, c3_scores, feature_importances):
    # feature_importances from XGBoost, normalized to sum to 1
    structural_weight = sum(feature_importances["structural_dims"])
    prompt_weight = sum(feature_importances["prompt_dims"])
    review_weight = 1 - structural_weight - prompt_weight

    weighted = (
        structural_weight * mean(c1_scores.values()) +
        prompt_weight * mean(c2_scores.values()) +
        review_weight * c3_scores["session_level"]
    )

    if weighted < 2.5:    label = "over_reliant"
    elif weighted <= 3.5: label = "balanced"
    else:                 label = "strategic"

    return weighted, label
```

**Fallback (no Component 1 artifact):** Equal weights across all three components.

**Upgrade path:** Replace weighted average with 2-layer MLP once real session data reaches 300+ labeled sessions.

### 5.7 LLM Judge Pass (Gemini, async)

Triggered asynchronously after synchronous scoring completes. Uses the same `GEMINI_API_KEY` and client as the AI assistant, but with a completely separate system prompt and model call — the judge prompt is structured around the scoring rubric, not code assistance.

**Input to judge:**

- All component scores + feature importances
- 3–5 selected session excerpts: worst-scoring prompt (lowest Component 2 score), lowest modification rate chunk, longest re-prompt sequence
- Session metadata: total time, phase breakdown, task description, `overall_label`, `fallback_components`

**Output:**

- 3–5 sentence behavioral summary
- Per-dimension narrative with specific session examples
- 2–3 concrete improvement suggestions
- Confidence flags where model certainty was low

Written to `session_scores.llm_narrative` and `session_scores.judge_chain_of_thought`.

---

## 6. Session Phases

| Phase | Expected behavior | Key events |
| --- | --- | --- |
| `orientation` | Reads problem, explores codebase. | `file_open`, `panel_focus` (filetree heavy), low `edit` count. |
| `implementation` | Active coding with AI suggestions. | `prompt`, `response`, `suggestion_shown`, `chunk_*`, `edit`, `execute`. |
| `verification` | Running tests, fixing bugs. | `execute` (high frequency), re-prompt sequences, low suggestion rate. |

---

## 7. Project Structure

```
backend/
├── app.py                  # Flask app factory, route registration, CORS setup
├── models.py               # SQLAlchemy models — all eight tables
├── db.py                   # Database init, session factory
├── scoring.py              # Scoring engine — model loading, Components 1–3, aggregation, judge
├── llm.py                  # LLM wrapper — Gemini Flash for both assistant and judge (separate system prompts)
├── diff.py                 # Unified diff generation (edit_delta) + hunk parsing (suggestions)
├── routes/
│   ├── session.py          # /session/start, /end, /phase, /trace
│   ├── files.py            # /files, /files/:id/save, /events/file
│   ├── ai.py               # /ai/chat  (Gemini Flash)
│   ├── suggestions.py      # /suggestions, /suggestions/:id/chunks/:idx/decide, /suggestions/:id
│   ├── events.py           # /events/editor, /events/execute, /events/panel
│   └── analytics.py        # /analytics/session/:id, /analytics/overview, /analytics/.../score
├── models/
│   ├── component1_xgboost.joblib          # Component 1 artifact (optional — fallback if absent)
│   ├── component2_lgbm.joblib             # Component 2 artifact (optional — fallback if absent)
│   └── component2_codebert_embeddings/    # Optional CodeBERT embeddings cache
├── .env                    # GEMINI_API_KEY, ANTHROPIC_API_KEY, DATABASE_URL, FLASK_SECRET_KEY
├── requirements.txt        # flask, flask-cors, sqlalchemy, google-generativeai,
│                           # python-dotenv, xgboost, lightgbm, joblib, scikit-learn
└── seed.py                 # Synthetic sessions with all v4 event types + chunk decisions
```

---

## 8. Environment Configuration & Setup

### 8.1 Environment Variables

| Variable | Required | Notes |
| --- | --- | --- |
| `GEMINI_API_KEY` | Yes | Powers both the AI assistant and LLM judge. Never committed to source. |
| `DATABASE_URL` | No | Defaults to `sqlite:///./sessions.db`. |
| `FLASK_SECRET_KEY` | Yes | Random string. |
| `GEMINI_MODEL_ID` | No | Defaults to `gemini-1.5-flash`. Used for both assistant and judge. |
| `MAX_TOKENS` | No | Defaults to `4096`. |
| `EDIT_DEBOUNCE_MS` | No | `2000`. Documented for frontend. |
| `SCORING_FALLBACK_MODE` | No | If `true`, forces rule-based heuristics even if model artifacts present. Useful for testing. |

### 8.2 Setup Steps

```bash
git clone <repo> && cd backend
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env          # fill in GEMINI_API_KEY
python db.py init             # creates sessions.db with all eight tables
# Optionally place trained model artifacts in models/ before running
# If models/ is empty, scoring engine runs in heuristic fallback mode
flask run --port 8000
```

---

## 9. Error Handling

| Scenario | HTTP status | Behavior |
| --- | --- | --- |
| Missing `X-Session-ID` | 401 | Required on all non-session-start endpoints. |
| Invalid `session_id` | 404 | Do not create implicit sessions. |
| Gemini API error | 502 | Do **NOT** write partial `ai_interactions` or `events` rows. |
| Gemini judge API error | 200 | Scoring pipeline result still written. `llm_narrative` set to null. Retry async. |
| Invalid `chunk_index` | 400 | Must exist in suggestion's hunk list. |
| Already-decided chunk | 409 | Each chunk decided once only. |
| `proposed_content == original_content` | 400 | No diff, no suggestion. Return error. |
| Dual-write transaction failure | 500 | Roll back both writes atomically. Log full traceback. |
| Hunk parsing failure | 500 | Do not create partial suggestion row. |
| Model artifact missing at score time | 200 | Score with heuristic fallback. Set `fallback_components` in response. Do not error — partial scoring beats no scoring. |
| Scoring pipeline failure | 500 | Log traceback. Do not write partial `session_scores` row. |

---

## 10. Open Questions & Post-Hackathon Decisions

| Question | Tonight | Post-hackathon |
| --- | --- | --- |
| Streaming Gemini responses | Synchronous. Parse into hunks on completion. | Stream explanation text; hold diff render until full response for clean hunk parse. |
| Accept All / Reject All | Frontend sends one decide request per chunk in sequence. | Add `POST /suggestions/:id/decide-all` batch endpoint. |
| Inline hunk editing | Frontend sends `modified` decision with `final_code`. Backend handles as-is. | No backend change needed — design already supports it. |
| Score caching invalidation | Re-run via manual `/score` endpoint. | Auto-invalidate `session_scores` when model artifacts are updated. |
| Partial suggestion timeout | Suggestions stay open until session ends; then auto-rejected. | Configurable timeout per suggestion. |
| CodeBERT embeddings | Skip if time-constrained — LightGBM runs on engineered features only. | Pre-compute and cache embeddings for all sessions post-training. |
| Authentication | Username-only input. Session ID as bearer token. | Proper auth before any real candidate data is collected. |
| Terminal sandboxing | Trust `exit_code` from frontend. | Docker execution environment with real test runner. |
| Concurrent sessions | SQLite fine for hackathon. | PostgreSQL for >10 simultaneous sessions. |
| Data retention | Keep everything. | Define policy before enterprise use. GDPR implications. |

---

> **Most important things to get right tonight:**
> 
> 1. **`POST /suggestions` hunk parsing** — the diff must be parsed into accurate hunks with correct line numbers. Everything in Component 3 and the dashboard depends on this.
> 2. **`POST /suggestions/:id/chunks/:idx/decide` atomicity** — chunk decision + dual-write to `events` in one transaction.
> 3. **`time_on_chunk_ms` timing discipline** — chunk 0: `decided_at − suggestion.shown_at`. Chunk N: `decided_at − previous_chunk.decided_at`. Client-side, sent with request.
> 4. **`final_code` discipline** — `accepted → proposed_code`, `rejected → original_code`, `modified → edited version`. Always actual code, never a flag.
> 5. **Dual LLM clients in `llm.py`** — Gemini for the assistant, Claude for the judge. Separate API keys, separate client instances, never mixed up.
> 6. **Scoring pipeline at session end** — Components 1–3 + aggregation must complete synchronously before `POST /session/end` returns. Judge runs async. `session_scores` row must be written atomically.